{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INL8yqnJPrQs"
      },
      "source": [
        "# Separação de partições com Pytorch sobre o dataset MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIhB73C4PzOq"
      },
      "outputs": [],
      "source": [
        "#importando  as bibliotecas\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jA2AJwQEPGut",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d5adad0-247f-491d-e98f-6a973af21457"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamanho do conjunto de treinamento: 48000 amostras\n",
            "Tamanho do conjunto de validação: 6000 amostras\n",
            "Tamanho do conjunto de teste: 6000 amostras\n"
          ]
        }
      ],
      "source": [
        "# Defina a transformação para normalizar os dados\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# Carregue o conjunto de dados MNIST\n",
        "mnist_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "# Defina a proporção de divisão entre treinamento, validação e teste\n",
        "train_ratio = 0.8\n",
        "val_ratio = 0.1\n",
        "test_ratio = 0.1\n",
        "\n",
        "# Calcule o tamanho dos conjuntos de treinamento, validação e teste\n",
        "train_size = int(train_ratio * len(mnist_dataset))\n",
        "val_size = int(val_ratio * len(mnist_dataset))\n",
        "test_size = len(mnist_dataset) - train_size - val_size\n",
        "\n",
        "# Divida o conjunto de dados em treinamento, validação e teste\n",
        "train_dataset, val_dataset, test_dataset = random_split(mnist_dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# Crie DataLoader para cada conjunto de dados\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Verifique o tamanho de cada conjunto\n",
        "print(f'Tamanho do conjunto de treinamento: {len(train_loader.dataset)} amostras')\n",
        "print(f'Tamanho do conjunto de validação: {len(val_loader.dataset)} amostras')\n",
        "print(f'Tamanho do conjunto de teste: {len(test_loader.dataset)} amostras')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Partições de treino e teste (ML vs DL)"
      ],
      "metadata": {
        "id": "7-qmvckCYf8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "IgT2qRloYnMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defina a transformação para normalizar os dados\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "#OUTRAS TRANSFORMAÇÕES POSSÍVEIS:\n",
        "\"\"\"\n",
        "transforms.RandomRotation(degrees=30) -> Rotaciona a imagem aleatoriamente até o máximo definido\n",
        "transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2) -> Altera caracteristicas da imagem dentro dos intervalos dados\n",
        "transforms.RandomHorizontalFlip(p=0.8) # Gira a imagem horizontalmente de forma randômica com base na probabilidade dada\n",
        "transforms.RandomCrop((crop_height, crop_width)) -> Realiza crops aleatorios\n",
        "transforms.Resize((new_height, new_width)) -> Aplica resize a todas imagens\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Carregue o conjunto de dados MNIST\n",
        "mnist_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "# Defina a proporção de divisão entre treinamento, validação e teste\n",
        "train_ratio = 0.8\n",
        "val_ratio = 0.1\n",
        "test_ratio = 0.1\n",
        "\n",
        "# Calcule o tamanho dos conjuntos de treinamento, validação e teste\n",
        "train_size = int(train_ratio * len(mnist_dataset))\n",
        "val_size = int(val_ratio * len(mnist_dataset))\n",
        "test_size = len(mnist_dataset) - train_size - val_size\n",
        "\n",
        "# Divida o conjunto de dados em treinamento, validação e teste\n",
        "train_dataset, val_dataset, test_dataset = random_split(mnist_dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# Crie DataLoader para cada conjunto de dados\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QI2vzPN2YtjY",
        "outputId": "2fa7f4c0-33a9-410e-a143-cd8d1003b8c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 88466300.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 77651085.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 30219533.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 8120429.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para treinar e avaliar uma Máquina de Vetores de Suporte (SVM)\n",
        "def train_and_evaluate_svm(train_loader, test_loader):\n",
        "    train_features = []\n",
        "    train_labels = []\n",
        "\n",
        "    for data in train_loader:\n",
        "        inputs, labels = data\n",
        "        train_features.append(inputs.view(inputs.size(0), -1).numpy())\n",
        "        train_labels.append(labels.numpy())\n",
        "\n",
        "    train_features = np.concatenate(train_features, axis=0)\n",
        "    train_labels = np.concatenate(train_labels, axis=0)\n",
        "\n",
        "    svm_classifier = svm.SVC(kernel='linear')\n",
        "    svm_classifier.fit(train_features, train_labels)\n",
        "\n",
        "    test_features = []\n",
        "    test_labels = []\n",
        "\n",
        "    for data in test_loader:\n",
        "        inputs, labels = data\n",
        "        test_features.append(inputs.view(inputs.size(0), -1).numpy())\n",
        "        test_labels.append(labels.numpy())\n",
        "\n",
        "    test_features = np.concatenate(test_features, axis=0)\n",
        "    test_labels = np.concatenate(test_labels, axis=0)\n",
        "\n",
        "    predictions = svm_classifier.predict(test_features)\n",
        "    accuracy = accuracy_score(test_labels, predictions)\n",
        "    print(f'SVM Accuracy: {accuracy}')"
      ],
      "metadata": {
        "id": "DuUZfKBTY1ft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para treinar e avaliar uma rede neural simples\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 128)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "def train_and_evaluate_nn(train_loader, val_loader, test_loader):\n",
        "    model = SimpleNN()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "    # Treinamento\n",
        "    for epoch in range(5):  # Número arbitrário de épocas para este exemplo\n",
        "        model.train()\n",
        "        for data in train_loader:\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validação\n",
        "        model.eval()\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        with torch.no_grad():\n",
        "            for data in val_loader:\n",
        "                inputs, labels = data\n",
        "                outputs = model(inputs)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_accuracy = val_correct / val_total\n",
        "        print(f'Epoch {epoch + 1}, Validation Accuracy: {val_accuracy}')\n",
        "\n",
        "    # Avaliação\n",
        "    model.eval()\n",
        "    test_correct = 0\n",
        "    test_total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            inputs, labels = data\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            test_total += labels.size(0)\n",
        "            test_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    test_accuracy = test_correct / test_total\n",
        "    print(f'Neural Network Test Accuracy: {test_accuracy}')"
      ],
      "metadata": {
        "id": "jRlRmTiTZCHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Treine e avalie SVM\n",
        "print(\"Avaliando Máquina de Vetores de Suporte (SVM):\")\n",
        "train_and_evaluate_svm(train_loader, test_loader)\n",
        "\n",
        "# Treine e avalie rede neural simples\n",
        "print(\"\\nAvaliando Rede Neural Simples:\")\n",
        "train_and_evaluate_nn(train_loader, val_loader, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMqX7ufqZXfE",
        "outputId": "02555146-9e9f-4100-bebd-d5dcb1cc08e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avaliando Máquina de Vetores de Suporte (SVM):\n",
            "SVM Accuracy: 0.9256666666666666\n",
            "\n",
            "Avaliando Rede Neural Simples:\n",
            "Epoch 1, Validation Accuracy: 0.9263333333333333\n",
            "Epoch 2, Validation Accuracy: 0.9543333333333334\n",
            "Epoch 3, Validation Accuracy: 0.9586666666666667\n",
            "Epoch 4, Validation Accuracy: 0.9628333333333333\n",
            "Epoch 5, Validation Accuracy: 0.9655\n",
            "Neural Network Test Accuracy: 0.9673333333333334\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}